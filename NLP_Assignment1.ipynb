{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment 1 — NLP PIPELINE"
      ],
      "metadata": {
        "id": "Zag3B4rpun_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1:Install required libraries"
      ],
      "metadata": {
        "id": "0R1IPli-usMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk scikit-learn pandas numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wJ9TlEh1rsBn",
        "outputId": "f3f6eeec-ecab-40c4-fb98-4d8bd49bf4b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2:IMPORT LIBRARIES"
      ],
      "metadata": {
        "id": "cgGA6POIsDo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5sDCewqusLLU",
        "outputId": "bafc7837-4904-4375-c005-f180c3788c63"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 3 — Enter a Custom Paragraph"
      ],
      "metadata": {
        "id": "KeMj4V2Qsi8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"\n",
        "NLP is an amazing field. It helps computers understand human language.\n",
        "We can perform tokenization, remove stopwords, and apply stemming and lemmatization.\n",
        "This helps in processing text for Machine Learning.\n",
        "\"\"\"\n",
        "print(\"Original Paragraph:\\n\", paragraph)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xH7E79yQsl4I",
        "outputId": "2a0accc8-c6ca-41b4-a13b-6c9a00a0a402"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Paragraph:\n",
            " \n",
            "NLP is an amazing field. It helps computers understand human language.\n",
            "We can perform tokenization, remove stopwords, and apply stemming and lemmatization.\n",
            "This helps in processing text for Machine Learning.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 4 — Tokenization"
      ],
      "metadata": {
        "id": "Ox_ha_KlstPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of splitting a paragraph into individual words or tokens.\n",
        "It helps the computer understand text word-by-word."
      ],
      "metadata": {
        "id": "UWtZeufsuNbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0FDBTMtstJY6",
        "outputId": "7fb63c08-51b8-4769-8565-0f824b5d638a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(paragraph)\n",
        "print(\"\\n--- TOKENIZATION RESULT ---\\n\")\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4neXW5eFsvMo",
        "outputId": "e37458b6-876d-47c3-b6f0-aecd2d26a8d0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TOKENIZATION RESULT ---\n",
            "\n",
            "['NLP', 'is', 'an', 'amazing', 'field', '.', 'It', 'helps', 'computers', 'understand', 'human', 'language', '.', 'We', 'can', 'perform', 'tokenization', ',', 'remove', 'stopwords', ',', 'and', 'apply', 'stemming', 'and', 'lemmatization', '.', 'This', 'helps', 'in', 'processing', 'text', 'for', 'Machine', 'Learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 5 — Stopword Removal"
      ],
      "metadata": {
        "id": "ljbkGrY4tSCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords are very common words like “the, is, am, are, of, in”.\n",
        "They don’t add meaning, so we remove them to keep only important words."
      ],
      "metadata": {
        "id": "0YxOJxRKuTg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(\"\\n--- AFTER STOPWORD REMOVAL ---\\n\")\n",
        "print(filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RrysGzVqtT37",
        "outputId": "0fd04c61-0ff1-4147-b252-67a9fd9c6aa7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AFTER STOPWORD REMOVAL ---\n",
            "\n",
            "['NLP', 'amazing', 'field', '.', 'helps', 'computers', 'understand', 'human', 'language', '.', 'perform', 'tokenization', ',', 'remove', 'stopwords', ',', 'apply', 'stemming', 'lemmatization', '.', 'helps', 'processing', 'text', 'Machine', 'Learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 6 — Stemming"
      ],
      "metadata": {
        "id": "WZLANtgvthYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming reduces words to a basic form by cutting the ends.\n",
        "Example:\n",
        "\n",
        "“playing” → “play”\n",
        "\n",
        "“studies” → “studi”\n",
        "\n",
        "It is fast but not always perfect."
      ],
      "metadata": {
        "id": "N89FPZDhuZdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "print(\"\\n--- STEMMING RESULT ---\\n\")\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lwY2E2v5tmS6",
        "outputId": "3c516241-17b1-4724-f3c5-c1a64c9947ae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- STEMMING RESULT ---\n",
            "\n",
            "['nlp', 'amaz', 'field', '.', 'help', 'comput', 'understand', 'human', 'languag', '.', 'perform', 'token', ',', 'remov', 'stopword', ',', 'appli', 'stem', 'lemmat', '.', 'help', 'process', 'text', 'machin', 'learn', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 7 — Lemmatization"
      ],
      "metadata": {
        "id": "BLO1p4DZtskw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization converts a word to its proper dictionary base form.\n",
        "Example:\n",
        "\n",
        "“better” → “good”\n",
        "\n",
        "“studies” → “study”\n",
        "\n",
        "It is more accurate than stemming because it uses vocabulary + grammar rules."
      ],
      "metadata": {
        "id": "Hlt5H9ZTue4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "print(\"\\n--- LEMMATIZATION RESULT ---\\n\")\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BcOkOyMGtxW7",
        "outputId": "9ac74b37-95ad-4efa-efe6-a498e9a2ef21"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LEMMATIZATION RESULT ---\n",
            "\n",
            "['NLP', 'amazing', 'field', '.', 'help', 'computer', 'understand', 'human', 'language', '.', 'perform', 'tokenization', ',', 'remove', 'stopwords', ',', 'apply', 'stemming', 'lemmatization', '.', 'help', 'processing', 'text', 'Machine', 'Learning', '.']\n"
          ]
        }
      ]
    }
  ]
}